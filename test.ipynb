{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 3, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "nets = random.sample([1, 2, 3, 4], 4)\n",
    "\n",
    "torch.stack(tuple(torch.tensor([net]) for net in nets), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from parameters import Parameters\n",
    "from core import replay_memory\n",
    "from core.mod_utils import is_lnorm_key\n",
    "import numpy as np\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "class GeneticAgent:\n",
    "    def __init__(self, args: Parameters):\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.actor = Actor(args)\n",
    "        self.old_actor = Actor(args)\n",
    "        self.temp_actor = Actor(args)\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=1e-4)\n",
    "\n",
    "        self.buffer = replay_memory.ReplayMemory(self.args.individual_bs, args.device)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def keep_consistency(self, z_old, z_new):\n",
    "        target_action = self.old_actor.select_action_from_z(z_old).detach()\n",
    "        current_action = self.actor.select_action_from_z(z_new)\n",
    "        delta = (current_action - target_action).abs()\n",
    "        dt = torch.mean(delta ** 2)\n",
    "        self.actor_optim.zero_grad()\n",
    "        dt.backward()\n",
    "        self.actor_optim.step()\n",
    "        return dt.data.cpu().numpy()\n",
    "\n",
    "    def keep_consistency_with_other_agent(self, z_old, z_new, other_actor):\n",
    "        target_action = other_actor.select_action_from_z(z_old).detach()\n",
    "        current_action = self.actor.select_action_from_z(z_new)\n",
    "        delta = (current_action - target_action).abs()\n",
    "        dt = torch.mean(delta ** 2)\n",
    "        self.actor_optim.zero_grad()\n",
    "        dt.backward()\n",
    "        self.actor_optim.step()\n",
    "        return dt.data.cpu().numpy()\n",
    "\n",
    "    def update_parameters(self, batch, p1, p2, critic):\n",
    "        state_batch, _, _, _, _ = batch\n",
    "\n",
    "        p1_action = p1(state_batch)\n",
    "        p2_action = p2(state_batch)\n",
    "        p1_q = critic.Q1(state_batch, p1_action).flatten()\n",
    "        p2_q = critic.Q1(state_batch, p2_action).flatten()\n",
    "\n",
    "        eps = 0.0\n",
    "        action_batch = torch.cat((p1_action[p1_q - p2_q > eps], p2_action[p2_q - p1_q >= eps])).detach()\n",
    "        state_batch = torch.cat((state_batch[p1_q - p2_q > eps], state_batch[p2_q - p1_q >= eps]))\n",
    "        actor_action = self.actor(state_batch)\n",
    "\n",
    "        # Actor Update\n",
    "        self.actor_optim.zero_grad()\n",
    "        sq = (actor_action - action_batch)**2\n",
    "        policy_loss = torch.sum(sq) + torch.mean(actor_action**2)\n",
    "        policy_mse = torch.mean(sq)\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        return policy_mse.item()\n",
    "\n",
    "class shared_state_embedding(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(shared_state_embedding, self).__init__()\n",
    "        self.args = args\n",
    "        l1 = 400\n",
    "        l2 = args.ls\n",
    "        l3 = l2\n",
    "\n",
    "        # Construct Hidden Layer 1\n",
    "        self.w_l1 = nn.Linear(args.state_dim, l1)\n",
    "        if self.args.use_ln: self.lnorm1 = LayerNorm(l1)\n",
    "\n",
    "        # Hidden Layer 2\n",
    "        self.w_l2 = nn.Linear(l1, l2)\n",
    "        if self.args.use_ln: self.lnorm2 = LayerNorm(l2)\n",
    "        # Init\n",
    "        self.to(self.args.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Hidden Layer 1\n",
    "        out = self.w_l1(state)\n",
    "        if self.args.use_ln: out = self.lnorm1(out)\n",
    "        out = out.tanh()\n",
    "\n",
    "        # Hidden Layer 2\n",
    "        out = self.w_l2(out)\n",
    "        if self.args.use_ln: out = self.lnorm2(out)\n",
    "        out = out.tanh()\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, args, init=False):\n",
    "        super(Actor, self).__init__()\n",
    "        self.args = args\n",
    "        l1 = args.ls; l2 = args.ls; l3 = l2\n",
    "        # Out\n",
    "        self.w_out = nn.Linear(l3, args.action_dim)\n",
    "        # Init\n",
    "        if init:\n",
    "            self.w_out.weight.data.mul_(0.1)\n",
    "            self.w_out.bias.data.mul_(0.1)\n",
    "\n",
    "        self.to(self.args.device)\n",
    "\n",
    "    def forward(self, input, state_embedding):\n",
    "        s_z = state_embedding.forward(input)\n",
    "        action = self.w_out(s_z).tanh()\n",
    "        return action\n",
    "\n",
    "    def select_action_from_z(self,s_z):\n",
    "\n",
    "        action = self.w_out(s_z).tanh()\n",
    "        return action\n",
    "\n",
    "    def select_action(self, state, state_embedding):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.args.device)\n",
    "        return self.forward(state, state_embedding).cpu().data.numpy().flatten()\n",
    "\n",
    "    def get_novelty(self, batch):\n",
    "        state_batch, action_batch, _, _, _ = batch\n",
    "        novelty = torch.mean(torch.sum((action_batch - self.forward(state_batch))**2, dim=-1))\n",
    "        return novelty.item()\n",
    "\n",
    "    # function to return current pytorch gradient in same order as genome's flattened parameter vector\n",
    "    def extract_grad(self):\n",
    "        tot_size = self.count_parameters()\n",
    "        pvec = torch.zeros(tot_size, dtype=torch.float32).to(self.args.device)\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if is_lnorm_key(name) or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            pvec[count:count + sz] = param.grad.view(-1)\n",
    "            count += sz\n",
    "        return pvec.detach().clone()\n",
    "\n",
    "    # function to grab current flattened neural network weights\n",
    "    def extract_parameters(self):\n",
    "        tot_size = self.count_parameters()\n",
    "        pvec = torch.zeros(tot_size, dtype=torch.float32).to(self.args.device)\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if is_lnorm_key(name) or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            pvec[count:count + sz] = param.view(-1)\n",
    "            count += sz\n",
    "        return pvec.detach().clone()\n",
    "\n",
    "    # function to inject a flat vector of ANN parameters into the model's current neural network weights\n",
    "    def inject_parameters(self, pvec):\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if is_lnorm_key(name) or len(param.shape) != 2:\n",
    "                continue\n",
    "            sz = param.numel()\n",
    "            raw = pvec[count:count + sz]\n",
    "            reshaped = raw.view(param.size())\n",
    "            param.data.copy_(reshaped.data)\n",
    "            count += sz\n",
    "\n",
    "    # count how many parameters are in the model\n",
    "    def count_parameters(self):\n",
    "        count = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if is_lnorm_key(name) or len(param.shape) != 2:\n",
    "                continue\n",
    "            count += param.numel()\n",
    "        return count\n",
    "\n",
    "class Q_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, args, l1=400, l2=300, l3=300):\n",
    "        super(Q_network, self).__init__()\n",
    "        self.args = args\n",
    "        self.l1 = l1\n",
    "        # Construct input interface (Hidden Layer 1)\n",
    "        self.w_l1 = nn.Linear(args.state_dim+args.action_dim, l1)\n",
    "        # Hidden Layer 2\n",
    "        self.w_l2 = nn.Linear(l1, l2)\n",
    "        if self.args.use_ln:\n",
    "            self.lnorm1 = LayerNorm(l1)\n",
    "            self.lnorm2 = LayerNorm(l2)\n",
    "        # Out\n",
    "        self.w_out = nn.Linear(l3, 1)\n",
    "        self.w_out.weight.data.mul_(0.1)\n",
    "        self.w_out.bias.data.mul_(0.1)\n",
    "        self.to(self.args.device)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # Hidden Layer 1 (Input Interface)\n",
    "        out = self.w_l1(input_)\n",
    "        if self.args.use_ln:out = self.lnorm1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        # Hidden Layer 2\n",
    "        out = self.w_l2(out)\n",
    "        if self.args.use_ln: out = self.lnorm2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        # Output interface\n",
    "        out = self.w_out(out)\n",
    "        return out\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, args, n_nets = 2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        l1 = 400;\n",
    "        l2 = 300;\n",
    "        l3 = l2\n",
    "\n",
    "        self.Q1_nets = []\n",
    "        self.Q2_nets = []\n",
    "\n",
    "        for i in range(n_nets):\n",
    "            self.Q1_network = Q_network(args, l1, l2, l3)\n",
    "            self.Q2_network = Q_network(args, l1, l2, l3)\n",
    "        \n",
    "            self.Q1_nets.append(self.Q1_network)\n",
    "            self.Q2_nets.append(self.Q2_network)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat((state, action), dim=-1)\n",
    "        quantiles_Q1 = torch.stack(tuple(net(sa) for net in self.Q1_nets), dim=1)\n",
    "        quantiles_Q2 = torch.stack(tuple(net(sa) for net in self.Q2_nets), dim=1)\n",
    "        return quantiles_Q1, quantiles_Q2\n",
    "        \n",
    "        \n",
    "    def Q1(self, input, action):\n",
    "\n",
    "        concat_input = torch.cat([input, action], -1)\n",
    "\n",
    "        out = self.w_l1(concat_input)\n",
    "        if self.args.use_ln:out = self.lnorm1(out)\n",
    "\n",
    "        out = F.leaky_relu(out)\n",
    "        # Hidden Layer 2\n",
    "        out = self.w_l2(out)\n",
    "        if self.args.use_ln: out = self.lnorm2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        # Output interface\n",
    "        out_1 = self.w_out(out)\n",
    "        return out_1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Policy_Value_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Policy_Value_Network, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.policy_size = self.args.ls * self.args.action_dim + self.args.action_dim\n",
    "\n",
    "        l1 = 400; l2 = 300; l3 = l2\n",
    "        self.l1 = l1\n",
    "        # Construct input interface (Hidden Layer 1)\n",
    "\n",
    "        if self.args.use_ln:\n",
    "            self.lnorm1 = LayerNorm(l1)\n",
    "            self.lnorm2 = LayerNorm(l2)\n",
    "            self.lnorm3 = LayerNorm(l1)\n",
    "            self.lnorm4 = LayerNorm(l2)\n",
    "        self.policy_w_l1 = nn.Linear(self.args.ls + 1, self.args.pr)\n",
    "        self.policy_w_l2 = nn.Linear(self.args.pr, self.args.pr)\n",
    "        self.policy_w_l3 = nn.Linear(self.args.pr, self.args.pr)\n",
    "\n",
    "        if self.args.OFF_TYPE == 1 :\n",
    "            input_dim = self.args.state_dim + self.args.action_dim\n",
    "        else:\n",
    "            input_dim = self.args.ls\n",
    "\n",
    "        self.w_l1 = nn.Linear(input_dim + self.args.pr, l1)\n",
    "        # Hidden Layer 2\n",
    "\n",
    "        self.w_l2 = nn.Linear(l1, l2)\n",
    "\n",
    "\n",
    "        # Out\n",
    "        self.w_out = nn.Linear(l3, 1)\n",
    "        self.w_out.weight.data.mul_(0.1)\n",
    "        self.w_out.bias.data.mul_(0.1)\n",
    "\n",
    "        self.policy_w_l4 = nn.Linear(self.args.ls + 1, self.args.pr)\n",
    "        self.policy_w_l5 = nn.Linear(self.args.pr, self.args.pr)\n",
    "        self.policy_w_l6 = nn.Linear(self.args.pr, self.args.pr)\n",
    "\n",
    "        self.w_l3 = nn.Linear(input_dim + self.args.pr, l1)\n",
    "        # Hidden Layer 2\n",
    "\n",
    "        self.w_l4 = nn.Linear(l1, l2)\n",
    "\n",
    "        # Out\n",
    "        self.w_out_2 = nn.Linear(l3, 1)\n",
    "        self.w_out_2.weight.data.mul_(0.1)\n",
    "        self.w_out_2.bias.data.mul_(0.1)\n",
    "\n",
    "        self.to(self.args.device)\n",
    "\n",
    "    def forward(self,  input,param):\n",
    "        reshape_param = param.reshape([-1,self.args.ls + 1])\n",
    "\n",
    "        out_p = F.leaky_relu(self.policy_w_l1(reshape_param))\n",
    "        out_p = F.leaky_relu(self.policy_w_l2(out_p))\n",
    "        out_p = self.policy_w_l3(out_p)\n",
    "        out_p = out_p.reshape([-1,self.args.action_dim,self.args.pr])\n",
    "        out_p = torch.mean(out_p,dim=1)\n",
    "\n",
    "        # Hidden Layer 1 (Input Interface)\n",
    "        concat_input = torch.cat((input,out_p), 1)\n",
    "\n",
    "        # Hidden Layer 2\n",
    "        out = self.w_l1(concat_input)\n",
    "        if self.args.use_ln: out = self.lnorm1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.w_l2(out)\n",
    "        if self.args.use_ln: out = self.lnorm2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        # Output interface\n",
    "        out_1 = self.w_out(out)\n",
    "\n",
    "        out_p = F.leaky_relu(self.policy_w_l4(reshape_param))\n",
    "        out_p = F.leaky_relu(self.policy_w_l5(out_p))\n",
    "        out_p = self.policy_w_l6(out_p)\n",
    "        out_p = out_p.reshape([-1, self.args.action_dim, self.args.pr])\n",
    "        out_p = torch.mean(out_p, dim=1)\n",
    "\n",
    "        # Hidden Layer 1 (Input Interface)\n",
    "        concat_input = torch.cat((input, out_p), 1)\n",
    "\n",
    "        # Hidden Layer 2\n",
    "        out = self.w_l3(concat_input)\n",
    "        if self.args.use_ln: out = self.lnorm3(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.w_l4(out)\n",
    "        if self.args.use_ln: out = self.lnorm4(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        # Output interface\n",
    "        out_2 = self.w_out_2(out)\n",
    "\n",
    "        \n",
    "        return out_1, out_2\n",
    "\n",
    "    def Q1(self, input, param):\n",
    "        reshape_param = param.reshape([-1, self.args.ls + 1])\n",
    "\n",
    "        out_p = F.leaky_relu(self.policy_w_l1(reshape_param))\n",
    "        out_p = F.leaky_relu(self.policy_w_l2(out_p))\n",
    "        out_p = self.policy_w_l3(out_p)\n",
    "        out_p = out_p.reshape([-1, self.args.action_dim, self.args.pr])\n",
    "        out_p = torch.mean(out_p, dim=1)\n",
    "\n",
    "        # Hidden Layer 1 (Input Interface)\n",
    "\n",
    "        # out_state = F.elu(self.w_state_l1(input))\n",
    "        # out_action = F.elu(self.w_action_l1(action))\n",
    "        concat_input = torch.cat((input, out_p), 1)\n",
    "\n",
    "        # Hidden Layer 2\n",
    "        out = self.w_l1(concat_input)\n",
    "        if self.args.use_ln: out = self.lnorm1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.w_l2(out)\n",
    "        if self.args.use_ln: out = self.lnorm2(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        # Output interface\n",
    "        out_1 = self.w_out(out)\n",
    "        return out_1\n",
    "\n",
    "import random\n",
    "\n",
    "def caculate_prob(score):\n",
    "\n",
    "    X = (score - np.min(score))/(np.max(score)-np.min(score) + 1e-8)\n",
    "    max_X = np.max(X)\n",
    "\n",
    "    exp_x = np.exp(X-max_X)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    prob = exp_x/sum_exp_x\n",
    "    return prob\n",
    "\n",
    "class TD3(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.max_action = 1.0\n",
    "        self.device = args.device\n",
    "        self.actor = Actor(args, init=True)\n",
    "        self.actor_target = Actor(args, init=True)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(args, n_nets=2).to(self.device)\n",
    "        self.critic_target = Critic(args, n_nets=2).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=1e-3)\n",
    "\n",
    "        self.buffer = replay_memory.ReplayMemory(args.individual_bs, args.device)\n",
    "\n",
    "\n",
    "        self.PVN = Policy_Value_Network(args).to(self.device)\n",
    "        self.PVN_Target = Policy_Value_Network(args).to(self.device)\n",
    "        self.PVN_Target.load_state_dict(self.PVN.state_dict())\n",
    "        self.PVN_optimizer = torch.optim.Adam([{'params': self.PVN.parameters()}],lr=1e-3)\n",
    "\n",
    "        self.state_embedding = shared_state_embedding(args)\n",
    "        self.state_embedding_target = shared_state_embedding(args)\n",
    "        self.state_embedding_target.load_state_dict(self.state_embedding.state_dict())\n",
    "      \n",
    "      \n",
    "        self.old_state_embedding = shared_state_embedding(args)\n",
    "        self.state_embedding_optimizer = torch.optim.Adam(self.state_embedding.parameters(), lr=1e-3)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def approximate_underestimate(self, std_target_Q, replay_buffer):\n",
    "        history_target_std = replay_buffer.update(self.args, std_target_Q)\n",
    "        history_target_std = torch.FloatTensor(history_target_std).unsqueeze(-1).to(self.device)\n",
    "        history_target_std_std, _ = torch.std_mean(history_target_std[:, 1:], 1)\n",
    "        over_target_std = torch.where(std_target_Q > 1, std_target_Q**0.9, std_target_Q)\n",
    "        if self.args.bellman_mode == \"TV\":\n",
    "            overstimate = torch.where((history_target_std_std > self.args.std_std_threshold), over_target_std, std_target_Q * 0)\n",
    "        elif self.args.bellman_mode == \"NV\":\n",
    "            overstimate = 0\n",
    "        elif self.args.bellman_mode == \"NT\":\n",
    "            overstimate = over_target_std\n",
    "        \n",
    "        return overstimate\n",
    "\n",
    "    def train(self,evo_times,all_fitness, all_gen , on_policy_states, on_policy_params, on_policy_discount_rewards,on_policy_actions,replay_buffer, iterations, batch_size=64, discount=0.99, tau=0.005, policy_noise=0.2,\n",
    "              noise_clip=0.5, policy_freq=2, train_OFN_use_multi_actor= False,all_actor = None):\n",
    "        actor_loss_list =[]\n",
    "        critic_loss_list =[]\n",
    "        pre_loss_list = []\n",
    "        pv_loss_list = [0.0]\n",
    "        keep_c_loss = [0.0]\n",
    "\n",
    "        for it in range(iterations):\n",
    "\n",
    "            x, y, u, r, d, _ ,_= replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(self.device)\n",
    "            action = torch.FloatTensor(u).to(self.device)\n",
    "            next_state = torch.FloatTensor(y).to(self.device)\n",
    "            done = torch.FloatTensor(1 - d).to(self.device)\n",
    "            reward = torch.FloatTensor(r).to(self.device)\n",
    "            \n",
    "            if self.args.EA:\n",
    "                if self.args.use_all:\n",
    "                    use_actors = all_actor\n",
    "                else :\n",
    "                    index = random.sample(list(range(self.args.pop_size+1)), 1)[0]\n",
    "                    use_actors = [all_actor[index]]\n",
    "\n",
    "                # off policy update\n",
    "                pv_loss = 0.0\n",
    "                for actor in use_actors:\n",
    "                    param = nn.utils.parameters_to_vector(list(actor.parameters())).data.cpu().numpy()\n",
    "                    param = torch.FloatTensor(param).to(self.device)\n",
    "                    param = param.repeat(len(state), 1)\n",
    "    \n",
    "                    with torch.no_grad():\n",
    "                        if self.args.OFF_TYPE == 1:\n",
    "                            input = torch.cat([next_state,actor.forward(next_state,self.state_embedding)],-1)\n",
    "                        else :\n",
    "                            input = self.state_embedding.forward(next_state)\n",
    "                        next_Q1, next_Q2 = self.PVN_Target.forward(input ,param)\n",
    "                        next_target_Q = torch.min(next_Q1,next_Q2)\n",
    "                        target_Q = reward + (done * discount * next_target_Q).detach()\n",
    "    \n",
    "                    if self.args.OFF_TYPE == 1:\n",
    "                        input = torch.cat([state,action], -1)\n",
    "                    else:\n",
    "                        input = self.state_embedding.forward(state)\n",
    "    \n",
    "                    current_Q1, current_Q2 = self.PVN.forward(input, param)\n",
    "                    pv_loss += F.mse_loss(current_Q1, target_Q)+ F.mse_loss(current_Q2, target_Q)\n",
    "    \n",
    "                self.PVN_optimizer.zero_grad()\n",
    "                pv_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.PVN.parameters(), 10)\n",
    "                self.PVN_optimizer.step()\n",
    "                pv_loss_list.append(pv_loss.cpu().data.numpy().flatten())\n",
    "            else :\n",
    "                pv_loss_list.append(0.0)\n",
    "\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = torch.FloatTensor(u).data.normal_(0, policy_noise).to(self.device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "\n",
    "            next_action = (self.actor_target.forward(next_state,self.state_embedding_target)+noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            std_target_Q1, mean_target_Q1 = torch.std_mean(target_Q1, 1)\n",
    "            std_target_Q2, mean_target_Q2 = torch.std_mean(target_Q2, 1)\n",
    "\n",
    "            underestimate_Q1 = self.approximate_underestimate(std_target_Q1, replay_buffer)\n",
    "            underestimate_Q2 = self.approximate_underestimate(std_target_Q2, replay_buffer)\n",
    "            \n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            breakpoint()\n",
    "            target_Q = reward + (done * discount * target_Q + max(underestimate_Q1, underestimate_Q2)).detach()\n",
    "            \n",
    "            # Get current Q estimates\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    " \n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), 10)\n",
    "            self.critic_optimizer.step()\n",
    "            critic_loss_list.append(critic_loss.cpu().data.numpy().flatten())\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % policy_freq == 0:\n",
    "\n",
    "                # Compute actor loss\n",
    "                s_z= self.state_embedding.forward(state)\n",
    "                actor_loss = -self.critic.Q1(state, self.actor.select_action_from_z(s_z)).mean()\n",
    "                # Optimize the actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), 10)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                if self.args.EA:\n",
    "                    index = random.sample(list(range(self.args.pop_size+1)), self.args.K)\n",
    "                    new_actor_loss = 0.0\n",
    "    \n",
    "                    if evo_times > 0 :\n",
    "                        for ind in index :\n",
    "                            actor = all_actor[ind]\n",
    "                            param = nn.utils.parameters_to_vector(list(actor.parameters())).data.cpu().numpy()\n",
    "                            param = torch.FloatTensor(param).to(self.device)\n",
    "                            param = param.repeat(len(state), 1)\n",
    "                            if self.args.OFF_TYPE == 1:\n",
    "                                input = torch.cat([state,actor.forward(state,self.state_embedding)], -1)\n",
    "                            else:\n",
    "                                input = self.state_embedding.forward(state)\n",
    "    \n",
    "                            new_actor_loss += -self.PVN.Q1(input,param).mean()\n",
    "\n",
    "\n",
    "                    total_loss = self.args.actor_alpha * actor_loss  + self.args.EA_actor_alpha* new_actor_loss\n",
    "                else :\n",
    "                    total_loss = self.args.actor_alpha * actor_loss\n",
    "\n",
    "                self.state_embedding_optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.state_embedding.parameters(), 10)\n",
    "                self.state_embedding_optimizer.step()\n",
    "                # Update the frozen target models\n",
    "                \n",
    "                for param, target_param in zip(self.state_embedding.parameters(), self.state_embedding_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                \n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.PVN.parameters(), self.PVN_Target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                actor_loss_list.append(actor_loss.cpu().data.numpy().flatten())\n",
    "                pre_loss_list.append(0.0)\n",
    "\n",
    "        return np.mean(actor_loss_list) , np.mean(critic_loss_list), np.mean(pre_loss_list),np.mean(pv_loss_list), np.mean(keep_c_loss)\n",
    "\n",
    "\n",
    "\n",
    "def fanin_init(size, fanin=None):\n",
    "    v = 0.008\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "def actfn_none(inp): return inp\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class OUNoise:\n",
    "\n",
    "    def __init__(self, action_dimension, scale=0.3, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.scale = scale\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/torch/autograd/graph.py:768: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/shaking/miniconda3/envs/phedarc/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_29924/112441356.py\", line 9, in <module>\n",
      "    z = x * w\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:111.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m w\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Second backward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This will cause the RuntimeError if retain_graph=True is not specified in the first backward call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/phedarc/lib/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phedarc/lib/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/phedarc/lib/python3.9/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define some dummy data and parameters\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "w = torch.tensor([0.1, 0.2, 0.3], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "z = x * w\n",
    "loss = z.sum()\n",
    "\n",
    "# First backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Modify w for demonstration\n",
    "w.data = w.data + 1.0\n",
    "\n",
    "# Second backward pass\n",
    "loss.backward()  # This will cause the RuntimeError if retain_graph=True is not specified in the first backward call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phedarc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
